{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "art_list = os.listdir('data/artigos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03-79-01-01port.txt\n"
     ]
    }
   ],
   "source": [
    "print(art_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = list(set().union(art_list))\n",
    "\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def preprocess_texts(all_files):\\n    corpus = []\\n    with open(os.path.join(dataset_path, all_files), \\'r\\', encoding=\\'latin-1\\') as text_file:\\n        sent_text_file = text_file.read()\\n        sent_text_file = (sent_tokenize(sent_text_file.lower(), language=\\'portuguese\\'))\\n        for sentence in sent_text_file:\\n            sentence = REPLACE_NO_SPACE.sub(\"\", sentence)\\n            sentence = REPLACE_WITH_SPACE.sub(\" \", sentence)\\n            sentence = word_tokenize(sentence, language=\\'portuguese\\')\\n            corpus.append(sentence)\\n            \\n    return corpus'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "dataset_path = 'dataset/artigos'\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for files in all_files:\n",
    "    with open(os.path.join(dataset_path, files), 'r', encoding='latin-1') as text_file:\n",
    "        sent_text_file = text_file.read()\n",
    "        sent_text_file = (sent_tokenize(sent_text_file.lower(), language='portuguese'))\n",
    "        for sentence in sent_text_file:\n",
    "            sentence = REPLACE_NO_SPACE.sub(\"\", sentence)\n",
    "            sentence = REPLACE_WITH_SPACE.sub(\" \", sentence)\n",
    "            sentence = word_tokenize(sentence, language='portuguese')\n",
    "            corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from multiprocessing import Pool\n",
    "#from math import floor\n",
    "\n",
    "#agents = 4\n",
    "#chunksize = floor(len(all_files)/4)\n",
    "#with Pool(processes=agents) as pool:\n",
    "#    processed_corpus = pool.map(preprocess_texts, all_files, chunksize)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[['este', 'trabalho', 'aborda', 'a', 'qualidade', 'da', 'geração', 'estatística', 'de', 'casos', 'de', 'teste', 'no', 'teste', 'estatístico', 'através', 'da', 'descrição', 'formal', 'de', 'programas'], ['a', 'principal', 'contribuição', 'consiste', 'na', 'proposta', 'de', 'uma', 'métrica', 'para', 'o', 'cálculo', 'de', 'índice', 'de', 'cobertura', 'baseado', 'em', 'trajetórias', 'ou', 'seja', 'é', 'formalizada', 'uma', 'métrica', 'que', 'verifica', 'a', 'relevância', 'de', 'conjuntos', 'de', 'casos', 'de', 'teste'], ['são', 'utilizadas', 'cadeias', 'de', 'markov', 'mc', 'e', 'redes', 'de', 'autômatos', 'estocásticos', 'san', 'como', 'métodos', 'formais', 'de', 'descrição', 'de', 'modelos', 'de', 'uso', 'no', 'sentido', 'de', 'verificar', 'vantagens', 'e', 'desvantagens', 'no', 'processo', 'de', 'teste', 'estatístico', 'utilizado'], ['no', 'decorrer', 'de', 'o', 'trabalho', 'discute', 'se', 'uma', 'análise', 'quantitativa', 'da', 'geração', 'de', 'casos', 'de', 'teste', 'bem', 'como', 'sua', 'eficiência', 'para', 'os', 'formalismos', 'mc', 'e', 'san'], ['como', 'resultado', 'são', 'apresentadas', 'uma', 'análise', 'quantitativa', 'da', 'geração', 'de', 'casos', 'de', 'teste', 'e', 'a', 'evolução', 'de', 'resultados', 'numéricos', 'para', 'a', 'métrica', 'proposta']]\n"
     ]
    }
   ],
   "source": [
    "#print(len(processed_corpus))\n",
    "print(type(corpus))\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(processed_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando word2vec\n",
    "\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "w2vmodel = Word2Vec(sentences=corpus,\n",
    "                    size=300,\n",
    "                    min_count=5,\n",
    "                    workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('residual', 0.7921961545944214),\n",
       " ('infiltração', 0.7854874134063721),\n",
       " ('líquido', 0.7786765098571777),\n",
       " ('mobilização', 0.775947093963623),\n",
       " ('metano', 0.7756657004356384),\n",
       " ('atmosfera', 0.7708741426467896),\n",
       " ('dissolvido', 0.7693185210227966),\n",
       " ('capilar', 0.7616554498672485),\n",
       " ('fria', 0.7561232447624207),\n",
       " 'vapor']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#palavras similares a:\n",
    "word = 'vapor'\n",
    "most_words = w2vmodel.wv.most_similar(word, topn=9)\n",
    "most_words.append(word)\n",
    "most_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computador', 0.4713716208934784),\n",
       " ('dom0', 0.47052323818206787),\n",
       " ('virtual', 0.46817874908447266),\n",
       " ('estudante', 0.467937171459198),\n",
       " ('hospedeira', 0.4617267847061157),\n",
       " ('qemu', 0.44181519746780396),\n",
       " ('programador', 0.4389238655567169),\n",
       " ('desenvolvedor', 0.42760422825813293),\n",
       " ('médico', 0.4185066819190979),\n",
       " ('hóspede', 0.4177491068840027)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#palavras similares com algumas restrições, sejam positivas ou negativas\n",
    "w2vmodel.wv.most_similar(positive=['criança', 'homem'], negative=['mulher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vetor de relacao da palavra\n",
    "#w2vmodel.wv.get_vector('muro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pont = []\n",
    "for proj in corpus:\n",
    "    cont = 0\n",
    "    for w in proj:\n",
    "        if w in most_words:\n",
    "            cont = cont +1\n",
    "    pont.append(cont/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(pont, reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
